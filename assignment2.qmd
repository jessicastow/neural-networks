---
title: "Assignment 2"
format: pdf
author: "Jessica Stow (STWJES003@MYUCT.AC.ZA)"
date: "14 October 2024"
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE,
                      include = FALSE,
                      warning = FALSE, 
                      cache = TRUE)
```

# Explore this report on my GitHub!

You can access the repository for this report on [my GitHub profile](https://github.com/jessicastow/neural-networks).

# Plagiarism declaration

-   I know that plagiarism is wrong.

-   Plagiarism is to use another’s work and pretend that it is one’s own.

-   I have used the required convention for citation and referencing.

-   Each contribution to and quotation in this assignment from the work(s) of other people has been attributed, and has been cited and referenced.

-   This assignment is my own work.

-   I have not allowed, and will not allow, anyone to copy my work with the intention of passing it off as his or her own work.

-   I acknowledge that copying someone else’s assignment or essay, or part of it, is wrong, and declare that this is my own work.

\newpage

```{r}
library(reticulate)
reticulate::use_virtualenv("r-keras", required = TRUE)  
library(tidyverse)
library(keras3)
library(ggplot2)
```

# Introduction to Neural Networks

Neural networks, the cornerstone of deep learning, encompass a large class of models and machine learning methods. A neural network is a multi-stage regression or classification model. They consist of an input layer, one or more hidden layers that utilise an activation function, which introduces non-linearity into the model to capture complex patterns in the data, and an output layer that produces the final prediction or classification result based on the learned representations (James et al. 2013).

# Multi-class Classification using Neural Networks

In this report, we utilise a neural network to perform multi-class classification and assess its suitability for the task. In neural networks, multi-class classification typically begins with an input layer, where the number of units corresponds to the number of features describing the data. This is followed by one or more hidden layers, which may include dense layers and dropout layers to prevent overfitting. Finally, the output layer contains a number of units equal to the possible classification classes, predicting the class to which each data point belongs.

## Objective

The objective of this task was to employ a neural network to predict a single target variable using a set of feature variables. The aim was to demonstrate the ability to apply neural networks as a suitable tool for addressing a classification problem.

## Data description

The dataset provided consisted of 21 numerical features, with the target variable being categorical and comprising five levels ("0", "1", "2", "3", and "4"). Limited information was available regarding the dataset features, which added a level of complexity when developing and fine-tuning the neural network to ensure accurate predictions.

```{r}
classification <- read.csv("data/Data-classification.csv")

str(classification[,1:21]) # Look at predictor variables

unique(classification$Target) # Look at target variable
```

## Exploratory data analysis

### Check for duplicates and null values

Checks were done for the presence of duplicate entries and null values. No duplicate entries were found and there were no null values present.

```{r}
sum(duplicated(classification)) # no duplicates

sum(is.na(classification)) # no nulls
```

### Feature Distribution and Standardisation Recommendation

The spread of the data for the 21 variables was visualised in *Figure 1* below. It was evident that the ranges of these values varied significantly, with differing scales and distributions. This disparity in feature ranges could have impacted the performance of machine learning models, particularly in the application of neural networks, which are sensitive to the scale of input features. As a result, it was recommended that these features later be standardised to have a mean of 0 and a standard deviation of 1. This would allow the neural network model to converge more efficiently and ensure that all features contributed equally.

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(classification[,1:21], 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 1: Boxplot of 21 features for classification task")
```

### Exploring Feature Differences by Target Category

```{r}
classification$Target <- as.factor(classification$Target)
# Create the faceted box plot
ggplot(classification, 
       aes(x = Target, 
           y = X10)) +
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(x = "Target", y = "X1", title = "Box Plot of X1 by Target Category") +
  theme_minimal()
```

### Check for imbalances in target variable

It was observed that the target variable was highly imbalanced, as shown below in **Figure 2**. Imbalanced classes are not ideal, as they can lead to the model becoming biased in its predictions, with the model favouring the dominant class. This imbalance is evident in the dataset, where class 2 is significantly overrepresented with 3478 observations, while class 3, with only 310 observations, is underrepresented. Such disparity can result in the model performing poorly when identifying the minority class, which in this case is class 3.

Other concerns related to imbalanced datasets include the challenge of generalisation, as models trained on skewed data often struggle to perform well on unseen data. Additionally, models can become more unstable and less reliable. To mitigate the bias towards the larger classes, particularly class 2, undersampling techniques were applied to balance the dataset and improve the model's performance across all classes.

```{r}
# Check the frequency distribution of the Target column
table(classification$Target)

# Check the proportion of each Target category
prop.table(table(classification$Target))

# Convert Target to a factor if it is not already
classification$Target <- as.factor(classification$Target)
```


```{r include=TRUE, out.width="90%"}
# Visualise the distribution of the Target variable
ggplot(classification, 
       aes(x = Target)) +
  geom_bar(fill = "skyblue3", 
           color = "black") +
  labs(x = "Target", 
       y = "Count", 
       title = "Figure 2: Distribution of Target Categories") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
```

## Data preparation

Once the data was loaded and explored, we applied the following preprocessing steps:

1.  **Addressing target variable imbalance through undersampling:** To correct the class imbalance in the target variable, we used undersampling.

-   We set a random seed for reproducibility, ensuring that the sampling process could be replicated for consistent results.

-   The dataset was balanced by randomly selecting 310 observations from each category, as this was the size of the smallest class.

-   By using undersampling, we created a balanced dataset where all target classes were equally represented. This approach helps improve the performance of our classification model by reducing bias towards the overrepresented classes.

```{r}
# Correct imbalance using undersampling
set.seed(123) # For reproducibility

# Balance the dataset so that each category in the Target has exactly 310 observations
classification_balanced <- classification %>%
  group_by(Target) %>% # Group by the Target variable
  sample_n(size = 310, replace = FALSE) %>%  # Sample 310 observations from each group, without replacement
  ungroup() # Ungroup to return a regular data frame
 
# Check the result to ensure each Target category has 310 observations
table(classification_balanced$Target) 
```

2.  **Creation of target variable and features:**

The target variable was separated from the feature set, where the target will serve as the predicted outcome, and the features will be used as inputs for modelling and further analysis.

```{r}
# Make target
classification_target <- classification_balanced$Target

# Make features
classification_features <- classification_balanced[,1:21]
```

4.  **Data split:** The dataset was split into training and testing sets using a 80-20 split ratio.

-   The data was split into the following shapes:

(a) Training set: 1248 observations, 21 features
(b) Testing set: 302 observations, 21 features

The training set would later be split into a training and validation set using a 80-20 split ratio. 

```{r}
# Determine sample size
set.seed(123) # for reproducibility
ind <- sample(1:2, 
              nrow(classification_balanced), 
              replace=TRUE, 
              prob=c(0.8, 0.2))

# Split features
x_train <- classification_features[ind==1, ]
x_test <- classification_features[ind==2, ]

# Split target
y_train <- classification_target[ind==1]
y_test <- classification_target[ind==2]

# Check size 
nrow(x_train) # 1248 observations
nrow(x_test) # 302 observations
```

5.  **Feature scaling:**

The features used in the training data was scaled to have a mean of 0 and a standard deviation of 1. The test data was then scaled based on the training data means and standard deviations.

```{r}
# Scale training data
x_train <- scale(x_train)

# Confirm means and std devs are now 0 and 1
apply(x_train, 2, mean)
apply(x_train, 2, sd)

attributes(x_train) # previous means and sds stored here

# Scale test data
x_test <- scale(x_test, 
                center = attr(x_train, "scaled:center"), 
                scale = attr(x_train, "scaled:scale"))

# Confirm means and std devs are now 0 and 1
apply(x_test, 2, mean)
apply(x_test, 2, sd)
```

6.  **One-hot encoding:** The integer-based target variable was transformed into a binary matrix using one-hot encoding, where each class is represented as its own binary variable, also known as "dummy variables".

```{r}
y_train <- keras3::to_categorical(y_train)
y_test_original <- y_test
y_test <- keras3::to_categorical(y_test)

# Check dimensions
dim(y_train)
```

## Multi-class classification model building

### Create and define model

A feedforward neural network was built to address this multi-class classification problem. This model consisted of an input layer, two dense layers and drop out layers, and one output layer. The layers used in this model were as follows:

-   The *input layer* had a shape of 21, since all 21 features were fed into the model as input.

-   The *first (hidden) dense layer* consisted of 64 neurons (units) and made use of the ReLU (Rectified Linear Unit) activation function - which introduces non-linearity into the model, allowing the model to learn complex patterns in the data it is trained on.

-   A *dropout layer* which randomly drops 20% of the neurons during training to prevent overfitting.

-   A *second dense layer* with 32 units and the ReLU activation function was then used. Having more than one dense layer enables the model to learn more complex representations of the data. The reason behind the reduction in the number of units (from 64 to 32) was to funnel the information learned by the model into a more precise form.

-   A *second dropout layer* was again used to reduce overfitting.

-   Finally, an *output layer* with 5 units, corresponding to the 5 classes in our target variable was used. For this layer, the softmax activation function was used since this is highly suitable for multi-class classification, since it is used to produce class probabilities. Softmax converts the output of the network into a probability distribution, where each of the 5 units will output a probability, and the sum of the probabilities across all units will equal 1. The class with the highest probability will be the predicted class.

Three different activation functions for classification

1.  Soft-max (for multi-class classification models ) -Output: Produces a probability distribution over multiple classes, where the sum of the probabilities is 1. -Layer: Typically applied in the final layer for multi-class classification tasks.

2.  Relu (hidden layers fir multiclass classification models) Output: Rectified Linear Unit outputs the input directly if it is positive; otherwise, it outputs zero. Layer: Not used in the output layer, but widely used in hidden layers to add non-linearity.

3.  tanh

Output: Produces values between -1 and 1, and is sometimes preferred over ReLU when the data is expected to have negative values or benefit from symmetric activation. Layer: Typically used in hidden layers but not the output layer.

Built a basic feed-forward neural network using Keras

```{r}
input <- layer_input(shape = c(21)) # input shape = 21 since we have 21 features

output <- input %>% 
    layer_dense(units = 64, activation = 'relu') %>% # can adjust units for complexity
    layer_dropout(rate = 0.2) %>% # can adjust drop out rate
    layer_dense(units = 32, activation = 'relu') %>%  # Additional layer
    layer_dropout(rate = 0.2) %>% 
    layer_dense(units = 5, # output for 5 classes (0-4)
                activation = 'softmax') # softmax activation function
```

The model was then defined by specifying the input and output layers. The intermediate dense and dropout layers were connected in a sequential manner.

```{r}
model <- keras_model(inputs = input, outputs = output)

summary(model)
```

### Compile model

The model was compiled in preparation for training for this multi-class classification task. The loss function used for measuring prediction error for multi-class classification was categorical crossentropy. The optimiser used was Adam, which optimises the model's weights using a learning rate of 0.001, helping to reduce the loss. The metric used to assess how well the model is predicting the correct class during traning and evaluation was accuracy.

```{r}
# Configure model for training
model %>% compile(
  loss = 'categorical_crossentropy', # loss function for multi-class classification
  # optimizer instance
  optimizer = optimizer_adam(learning_rate = 0.001),
  # evaluate the model (during training & testing) based on the following metrics:
  metrics =  c('accuracy')
)
```

### Model training

```{r}
# Shuffle the data manually before splitting into training and validation
set.seed(123)  # For reproducibility
shuffled_indices <- sample(1:nrow(x_train))

x_train <- x_train[shuffled_indices, ]
y_train <- y_train[shuffled_indices, ]

history <- model %>% fit(
  x_train, y_train, 
  epochs = 50, # nr of times model trains on the entire data set
  batch_size = 50, # nr of samples processed before updating the model
  validation_split = 0.2, # using 20% of data for validation
  shuffle = TRUE, # shuffle training data before each epoch
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 10))
)

plot(history)
```

### Model evaluation

Evaluate model perfromace on test data

```{r}
model %>% evaluate(x_test, y_test)
```

Confusion matrix

```{r}
y_test_hat <- model %>% 
  predict(x_test) %>% 
  op_argmax(axis=2) %>% 
  as.numeric()

table(y_test_original, y_test_hat)
```

Append predictions

```{r}
# edit this code 
#iris_testset <- classification %>% 
 # mutate(testset = ind == 2) %>%
 # filter(testset)

#iris_testset <- iris_testset %>%
 # mutate(obs_class = y_test_original, pred_class = y_test_hat)

#wrong_predictions <- iris_testset %>% filter(pred_class != obs_class)
#head(wrong_predictions)

```

## Recommendations

To address the class imbalance of the target variable we employed undersampling. Alternative methods: oversampling by duplicating or generating synthetic data (SMOTE), assigning class weights to emphasise minority classes during training.

# Regression task

```{r}
regression <- read.csv("data/Data-regression.csv")
str(regression)
```

```{r}
regression$target
```

```{r}
head(regression)
str(regression)
```
## EDA

### Check for duplicates and null values

Checks were done for the presence of duplicate entries and null values. 25 duplicate entries were found and removed, and there were no null values present.

```{r}
sum(duplicated(regression)) # 25 duplicates

# drop duplicates
regression <- regression[!duplicated(regression), ]

# check duplicates were dropped
sum(duplicated(regression)) # 0 duplicates

sum(is.na(regression)) # no nulls
```

### Feature Distribution and Standardisation Recommendation

The spread of the data for the 8 variables was visualised in *Figure 2* below. It was evident that the ranges of these values varied significantly, with differing scales and distributions. This disparity in feature ranges could have impacted the performance of machine learning models, particularly in the application of neural networks, which are sensitive to the scale of input features. As a result, it was recommended that these features later be standardised to have a mean of 0 and a standard deviation of 1. This would allow the neural network model to converge more efficiently and ensure that all features contributed equally.

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(regression[,1:8], 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 2: Boxplot of 8 features for regression task")
```

### Spread of target variable

Have values across a wide range (2.33 to 82.6) which include a handful of outliers on the upper end of the distribution. We have no ctext for this target variable. 

```{r}
min(regression$target)
max(regression$target)
```

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(regression$target, 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 2: Boxplot of target variable")
```

## Data preparation

1. Create features and target

```{r}
regression_features <- regression[,1:8]
regression_target <- regression$target
```

**Data split:** The dataset was split into training and testing sets using a 80-20 split ratio.

-   The data was split into the following shapes:

(a) Training set: 850 observations, 8 features
(b) Testing set: 200 observations, 8 features

```{r}
# Determine sample size
set.seed(123) # for reproducibility
ind <- sample(1:2, 
              nrow(regression), 
              replace=TRUE, 
              prob=c(0.8, 0.2))

# Split features
x_train_r <- regression_features[ind==1, ]
x_test_r <- regression_features[ind==2, ]

# Split target
y_train_r <- regression_target[ind==1]
y_test_r <- regression_target[ind==2]

# Check size 
nrow(x_train_r) # 850 observations
nrow(x_test_r) # 200 observations
```

**Feature scaling:**

The features used in the training data was scaled to have a mean of 0 and a standard deviation of 1. The test data was then scaled based on the training data means and standard deviations.

```{r}
# Scale training data
x_train_r <- scale(x_train_r)

# Confirm means and std devs are now 0 and 1
apply(x_train_r, 2, mean)
apply(x_train_r, 2, sd)

attributes(x_train_r) # previous means and sds stored here

# Scale test data
x_test_r <- scale(x_test_r, 
                center = attr(x_train_r, "scaled:center"), 
                scale = attr(x_train_r, "scaled:scale"))

# Confirm means and std devs are now 0 and 1
apply(x_test_r, 2, mean)
apply(x_test_r, 2, sd)
```

## Model building

### Create and define model

```{r}
input_r <- layer_input(shape = c(8))

output_r <- input_r %>% 
    layer_dense(units = 64, activation = 'relu') %>% 
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 32, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 1, activation = 'linear')
```


```{r}
model_r <- keras_model(inputs = input_r, 
                       outputs = output_r)

summary(model_r)
```

Compile model

```{r}
model_r %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('mean_squared_error'),
)
```

Model training

```{r}
# Shuffle the data manually before splitting into training and validation
set.seed(123)  # For reproducibility
shuffled_indices_r <- sample(1:nrow(x_train_r))

x_train_r <- x_train_r[shuffled_indices_r, ]
y_train_r <- y_train_r[shuffled_indices_r]

history_r <- model_r %>% fit(
  as.matrix(x_train_r), 
  as.matrix(y_train_r), 
  epochs = 50, 
  batch_size = 32, 
  validation_split = 0.2, 
  shuffle = TRUE,
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 10))
)
```
```{r}
y_test_hat_r <- model_r %>% predict(x_test_r)

preds_r <- data.frame(true_value = y_test_r, pred_value = y_test_hat_r)

preds_r <- preds_r %>% 
  mutate(abs_error = abs(true_value - pred_value))

preds_r
```
Evaluate model

```{r}
hist(preds_r$abs_error)
```

```{r}
options(repr.plot.width = 20, repr.plot.height = 8)

ggplot(preds_r, 
       aes(x = true_value, 
           y = pred_value)) +
    geom_point(size = 2) +
  labs(title = "Predicted value vs True value",
       x = "True value",
       y = "Predicted value") +
  theme_minimal() + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red")
```

# References

James, G., Witten, D., Hastie, T. and Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. New York: Springer. 
