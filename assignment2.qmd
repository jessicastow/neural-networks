---
title: "STA5073 Assignment 2: Neural Networks"
format: pdf
author: "Jessica Stow (STWJES003@MYUCT.AC.ZA)"
date: "14 October 2024"
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE,
                      include = FALSE,
                      warning = FALSE)
```

# View this report on my GitHub!

You can access the repository for this report on [my GitHub profile](https://github.com/jessicastow/neural-networks).

# Plagiarism declaration

-   I know that plagiarism is wrong.

-   Plagiarism is to use another’s work and pretend that it is one’s own.

-   I have used the required convention for citation and referencing.

-   Each contribution to and quotation in this assignment from the work(s) of other people has been attributed, and has been cited and referenced.

-   This assignment is my own work.

-   I have not allowed, and will not allow, anyone to copy my work with the intention of passing it off as his or her own work.

-   I acknowledge that copying someone else’s assignment or essay, or part of it, is wrong, and declare that this is my own work.

\newpage

```{r}
library(reticulate)
reticulate::use_virtualenv("r-keras", required = TRUE)  
library(tidyverse)
library(keras3)
library(ggplot2)
```

# Introduction to Neural Networks

Neural networks, the cornerstone of deep learning, encompass a large class of models and machine learning methods. A neural network is a two-stage regression or classification model. They consist of an input layer, one or more hidden layers that utilise an activation function, which introduces non-linearity into the model to capture complex patterns in the data, and an output layer that produces the final prediction or classification result based on the learned representations (James et al. 2013).

# Multi-class Classification using Neural Networks

In this report, we utilise a neural network to perform multi-class classification and assess its suitability for the task. In neural networks, multi-class classification typically begins with an input layer, where the number of units corresponds to the number of features describing the data. This is followed by one or more hidden layers, which may include dense layers and dropout layers to prevent overfitting. Finally, the output layer contains a number of units equal to the possible classification classes, predicting the class to which each data point belongs.

## Objective

The objective of this task was to employ a neural network to predict a single target variable using a set of feature variables. The aim was to demonstrate the ability to apply neural networks as a suitable tool for addressing a classification problem.

## Data description

The dataset provided consisted of 21 numerical features, with the target variable being categorical and comprising five levels ("0", "1", "2", "3", and "4"). Limited information was available regarding the dataset features, which added a level of complexity when developing and fine-tuning the neural network to ensure accurate predictions.

```{r}
classification <- read.csv("data/Data-classification.csv")

str(classification[,1:21]) # Look at predictor variables

unique(classification$Target) # Look at target variable
```

\newpage

## Exploratory data analysis

### Check for duplicates and null values

Checks were done for the presence of duplicate entries and null values. No duplicate entries were found and there were no null values present.

```{r}
sum(duplicated(classification)) # no duplicates

sum(is.na(classification)) # no nulls
```

### Feature Distribution and Standardisation Recommendation

The spread of the data for the 21 variables was visualised in *Figure 1* below. It was evident that the ranges of these values varied significantly, with differing scales and distributions. This disparity in feature ranges could have impacted the performance of machine learning models, particularly in the application of neural networks, which are sensitive to the scale of input features. As a result, it was recommended that these features later be standardised to have a mean of 0 and a standard deviation of 1. This would allow the neural network model to converge more efficiently and ensure that all features contributed equally.

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(classification[,1:21], 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 1: Boxplot of 21 features for classification task")
```

### Exploring Feature Differences by Target Category

```{r}
classification$Target <- as.factor(classification$Target)
# Create the faceted box plot
ggplot(classification, 
       aes(x = Target, 
           y = X10)) +
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(x = "Target", y = "X1", title = "Box Plot of X1 by Target Category") +
  theme_minimal()
```

### Check for imbalances in target variable

It was observed that the target variable was highly imbalanced, as shown below in **Figure 2**. Imbalanced classes are not ideal, as they can lead to the model becoming biased in its predictions, with the model favouring the dominant class. This imbalance is evident in the dataset, where class 2 is significantly overrepresented with 3478 observations, while class 3, with only 310 observations, is underrepresented. Such disparity can result in the model performing poorly when identifying the minority class, which in this case is class 3.

Other concerns related to imbalanced datasets include the challenge of generalisation, as models trained on skewed data often struggle to perform well on unseen data. Additionally, models can become more unstable and less reliable. To mitigate the bias towards the larger classes, particularly class 2, undersampling techniques were applied to balance the dataset and improve the model's performance across all classes.

```{r}
# Check the frequency distribution of the Target column
table(classification$Target)

# Check the proportion of each Target category
prop.table(table(classification$Target))

# Convert Target to a factor if it is not already
classification$Target <- as.factor(classification$Target)
```

```{r include=TRUE, out.width="90%"}
# Visualise the distribution of the Target variable
ggplot(classification, 
       aes(x = Target)) +
  geom_bar(fill = "skyblue3", 
           color = "black") +
  labs(x = "Target", 
       y = "Count", 
       title = "Figure 2: Distribution of Target Categories") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
```

\newpage

## Data preparation

Once the data was loaded and explored, we applied the following preprocessing steps:

1.  **Addressing target variable imbalance through undersampling:** To correct the class imbalance in the target variable, we used undersampling.

-   We set a random seed for reproducibility, ensuring that the sampling process could be replicated for consistent results.

-   The dataset was balanced by randomly selecting 310 observations from each category, as this was the size of the smallest class.

-   By using undersampling, we created a balanced dataset where all target classes were equally represented. This approach helps improve the performance of our classification model by reducing bias towards the overrepresented classes.

```{r}
# Correct imbalance using undersampling
set.seed(123) # For reproducibility

# Balance the dataset so that each category in the Target has exactly 310 observations
classification_balanced <- classification %>%
  group_by(Target) %>% # Group by the Target variable
  sample_n(size = 310, replace = FALSE) %>%  # Sample 310 observations from each group, without replacement
  ungroup() # Ungroup to return a regular data frame
 
# Check the result to ensure each Target category has 310 observations
table(classification_balanced$Target) 
```

2.  **Creation of target variable and features:**

-   The target variable was separated from the feature set, where the target will serve as the predicted outcome, and the features will be used as inputs for modelling and further analysis.

```{r}
# Make target
classification_target <- classification_balanced$Target

# Make features
classification_features <- classification_balanced[,1:21]
```

4.  **Data split:** The dataset was split into training and testing sets using a 80-20 split ratio.

-   The data was split into the following shapes:

(a) Training set: 1248 observations, 21 features
(b) Testing set: 302 observations, 21 features

-   The training set would later be split into a training and validation set using a 80-20 split ratio.

```{r}
# Determine sample size
set.seed(123) # for reproducibility
ind <- sample(1:2, 
              nrow(classification_balanced), 
              replace=TRUE, 
              prob=c(0.8, 0.2))

# Split features
x_train <- classification_features[ind==1, ]
x_test <- classification_features[ind==2, ]

# Split target
y_train <- classification_target[ind==1]
y_test <- classification_target[ind==2]

# Check size 
nrow(x_train) # 1248 observations
nrow(x_test) # 302 observations
```

5.  **Feature scaling:**

-   The features used in the training data was scaled to have a mean of 0 and a standard deviation of 1. The test data was then scaled based on the training data means and standard deviations.

```{r}
# Scale training data
x_train <- scale(x_train)

# Confirm means and std devs are now 0 and 1
apply(x_train, 2, mean)
apply(x_train, 2, sd)

attributes(x_train) # previous means and sds stored here

# Scale test data
x_test <- scale(x_test, 
                center = attr(x_train, "scaled:center"), 
                scale = attr(x_train, "scaled:scale"))

# Confirm means and std devs are now 0 and 1
apply(x_test, 2, mean)
apply(x_test, 2, sd)
```

6.  **One-hot encoding:**

-   The integer-based target variable was transformed into a binary matrix using one-hot encoding, where each class is represented as its own binary variable, also known as "dummy variables".

```{r}
y_train <- keras3::to_categorical(y_train)
y_test_original <- y_test
y_test <- keras3::to_categorical(y_test)

# Check dimensions
dim(y_train)
```

## Multi-class classification model building

### Create and define model

A feedforward neural network was built to address this multi-class classification problem. This model consisted of an input layer, two dense layers and drop out layers, and one output layer. The layers used in this model were as follows:

-   The **input layer** had a shape of 21, since all 21 features were fed into the model as input.

-   The **first (hidden) dense layer** consisted of 64 neurons (units) and made use of the ReLU (Rectified Linear Unit) activation function - which introduces non-linearity into the model, allowing the model to learn complex patterns in the data it is trained on.

-   A **dropout layer** which randomly drops 20% of the neurons during training to prevent overfitting.

-   A **second dense layer** with 32 units and the ReLU activation function was then used. Having more than one dense layer enables the model to learn more complex representations of the data. The reason behind the reduction in the number of units (from 64 to 32) was to funnel the information learned by the model into a more precise form.

-   A **second dropout layer** was again used to reduce overfitting.

-   Finally, an **output layer** with 5 units, corresponding to the 5 classes in our target variable was used. For this layer, the softmax activation function was used since this is highly suitable for multi-class classification, since it is used to produce class probabilities. Softmax converts the output of the network into a probability distribution, where each of the 5 units will output a probability, and the sum of the probabilities across all units will equal 1. The class with the highest probability will be the predicted class.

```{r}
input <- layer_input(shape = c(21)) # input shape = 21 since we have 21 features

output <- input %>% 
    layer_dense(units = 64, activation = 'relu') %>% # can adjust units for complexity
    layer_dropout(rate = 0.2) %>% # can adjust drop out rate
    layer_dense(units = 32, activation = 'relu') %>%  # Additional layer
    layer_dropout(rate = 0.2) %>% 
    layer_dense(units = 5, # output for 5 classes (0-4)
                activation = 'softmax') # softmax activation function
```

The model was then defined by specifying the input and output layers. The intermediate dense and dropout layers were connected in a sequential manner.

```{r}
model <- keras_model(inputs = input, outputs = output)

summary(model)
```

### Model compilation

The model was compiled in preparation for the multi-class classification task. Categorical crossentropy was selected as the loss function to measure prediction error, as it is suited for multi-class problems. The Adam optimiser was used to adjust the model's weights, with a learning rate of 0.001, to minimise the loss function. Accuracy was chosen as the evaluation metric to assess the model's performance in predicting the correct class during both training and evaluation.

```{r}
# Configure model for training
model %>% compile(
  loss = 'categorical_crossentropy', # loss function for multi-class classification
  optimizer = optimizer_adam(learning_rate = 0.001), # optimizer instance = adam
  # evaluate the model (during training & testing) based on the following metrics:
  metrics =  c('accuracy')
)
```

### Model training

The data was first manually shuffled before splitting it into training and validation sets. This ensured the data was randomised. A set seed was used for reproducibility of the manual shuffle.

```{r}
# Shuffle the data manually before splitting into training and validation
set.seed(123)  # For reproducibility
shuffled_indices <- sample(1:nrow(x_train))

x_train <- x_train[shuffled_indices, ]
y_train <- y_train[shuffled_indices, ]
```

Once the training data had been shuffled, the model was trained using the `fit()` function. The model trained over the entire data set 50 times (number of epochs set to 50). This was done by dividing the training data into batches of 50 samples, and once the model had processed each batch, the model's weights were updated. This was done to help the model learn more efficiently as the updates to weights could be made more frequently instead of waiting for the entire data set to be trained on. Before each epoch, the training data was shuffled to ensure the model did not learn any patterns based on the order of the data.

20% of the data of the training data was used for validation during training. This allowed the model to be evaluated on unseen data after each epoch in order to monitor its performance and prevent overfitting.

Finally, an early stopping mechanism was included to monitor validation loss. Should the validation loss not improve for 10 consecutive epochs, the training will stop early. This is done to prevent overfitting and to save time by avoiding any unnecessary model training that does not improve the performance of the model.

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 50, # nr of times model trains on the entire data set
  batch_size = 50, # nr of samples processed before updating the model
  validation_split = 0.2, # using (the last) 20% of data for validation
  shuffle = TRUE, # shuffle training data before each epoch
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 10))
)

history # final
```

```{r include = TRUE, out.width="90%"}
# Manually add title later "Figure 3: Model Training and Validation Performance"
plot(history)
```

### Model performance evaluation

The trained model's performance was evaluated on the test data.

The confusion matrix for the model predictions versus true classifications are displayed below in *Table 1*:

#### Table 1: Confusion Matrix

| **True Label / Predicted Label** | **0** | **1** | **2** | **3** | **4** |
|----------------------------------|-------|-------|-------|-------|-------|
| **0**                            | 55    | 1     | 0     | 0     | 0     |
| **1**                            | 0     | 69    | 0     | 1     | 0     |
| **2**                            | 0     | 0     | 59    | 0     | 0     |
| **3**                            | 2     | 0     | 1     | 54    | 0     |
| **4**                            | 0     | 0     | 0     | 0     | 60    |

\* *Note: rows represent true labels and columns represent predicted labels.*

Only 5 out of the 302 test predictions were misclassified, allowing for a high accuracy score of 98%.

The misclassifications for our test set were as follows:

-   For class 0, there was one observation misclassified as class 1.
-   For class 1, there was one observation misclassified as class 3.
-   For class 3, there were three observations misclassified. Two of the observations were misclassified as class 0 and one was misclassified as class 2. This class had the most misclassifications of all 5 classes.

For classes 2 and 4 there were no misclassifications.

```{r}
model %>% evaluate(x_test, y_test)

y_test_hat <- model %>% 
  predict(x_test) %>% 
  op_argmax(axis=2) %>% 
  as.numeric()

table(y_test_original, y_test_hat) # display confusion matrix
```

\newpage 

## Discussion

The implementation of a neural network for this multi-class classification challenge proved to be highly effective, as shown by the model's high accuracy scores on both the training and testing datasets. Following data preprocessing — which included addressing class imbalances and normalising the feature set — the model achieved an accuracy of 98% on the test set. This strong performance indicates the model's ability to generalise well on unseen data.

The confusion matrix further illustrates that the model performed well across all five classes, with only five misclassifications out of 302 test observations. Notably, the majority of misclassification errors were in class 3, which was the most underrepresented class in the original dataset. This suggests that while the undersampling method helped to balance the dataset, the model may have struggled to learn the distinguishing patterns associated with class 3 due to its limited representation.

The architecture of the neural network, which used ReLU activation functions in the hidden layers and a softmax activation function in the output layer, was well-suited for this classification task. The inclusion of dropout layers effectively mitigated overfitting, as demonstrated by stable performance on the validation set throughout training. Additionally, implementing early stopping ensured that the model did not overtrain, resulting in a more robust final model.

## Conclusion

In conclusion, the neural network model developed for this multi-class classification problem exhibited exceptional performance, achieving a high accuracy score and minimal misclassifications. The preprocessing steps, particularly those aimed at balancing the dataset through undersampling and feature scaling, are believed to have contributed to the model's effectiveness. However, some misclassifications persisted, particularly within the underrepresented class 3. This highlights an opportunity for further refinement in handling class imbalances.

## Recommendations

To further improve the model’s performance, it is recommended to explore alternative methods for addressing class imbalances. Techniques such as oversampling or synthetic data generation methods, such as SMOTE, could improve the model’s ability to effectively classify minority classes. Additionally, experimenting with different models, activation functions or hyperparameter tuning may yield further improvements. Overall, the results demonstrate the effectiveness and flexibility of neural networks in tackling complex classification tasks, even when faced with challenges such as data imbalance.

\newpage

# Regression task

```{r}
regression <- read.csv("data/Data-regression.csv")
str(regression)
```

```{r}
regression$target
```

```{r}
head(regression)
str(regression)
```

## EDA

### Check for duplicates and null values

Checks were done for the presence of duplicate entries and null values. 25 duplicate entries were found and removed, and there were no null values present.

```{r}
sum(duplicated(regression)) # 25 duplicates

# drop duplicates
regression <- regression[!duplicated(regression), ]

# check duplicates were dropped
sum(duplicated(regression)) # 0 duplicates

sum(is.na(regression)) # no nulls
```

### Feature Distribution and Standardisation Recommendation

The spread of the data for the 8 variables was visualised in *Figure 2* below. It was evident that the ranges of these values varied significantly, with differing scales and distributions. This disparity in feature ranges could have impacted the performance of machine learning models, particularly in the application of neural networks, which are sensitive to the scale of input features. As a result, it was recommended that these features later be standardised to have a mean of 0 and a standard deviation of 1. This would allow the neural network model to converge more efficiently and ensure that all features contributed equally.

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(regression[,1:8], 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 2: Boxplot of 8 features for regression task")
```

### Spread of target variable

Have values across a wide range (2.33 to 82.6) which include a handful of outliers on the upper end of the distribution. We have no ctext for this target variable.

```{r}
min(regression$target)
max(regression$target)
```

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(regression$target, 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 2: Boxplot of target variable")
```

## Data preparation

1.  Create features and target

```{r}
regression_features <- regression[,1:8]
regression_target <- regression$target
```

**Data split:** The dataset was split into training and testing sets using a 80-20 split ratio.

-   The data was split into the following shapes:

(a) Training set: 850 observations, 8 features
(b) Testing set: 200 observations, 8 features

```{r}
# Determine sample size
set.seed(123) # for reproducibility
ind <- sample(1:2, 
              nrow(regression), 
              replace=TRUE, 
              prob=c(0.8, 0.2))

# Split features
x_train_r <- regression_features[ind==1, ]
x_test_r <- regression_features[ind==2, ]

# Split target
y_train_r <- regression_target[ind==1]
y_test_r <- regression_target[ind==2]

# Check size 
nrow(x_train_r) # 850 observations
nrow(x_test_r) # 200 observations
```

**Feature scaling:**

The features used in the training data was scaled to have a mean of 0 and a standard deviation of 1. The test data was then scaled based on the training data means and standard deviations.

```{r}
# Scale training data
x_train_r <- scale(x_train_r)

# Confirm means and std devs are now 0 and 1
apply(x_train_r, 2, mean)
apply(x_train_r, 2, sd)

attributes(x_train_r) # previous means and sds stored here

# Scale test data
x_test_r <- scale(x_test_r, 
                center = attr(x_train_r, "scaled:center"), 
                scale = attr(x_train_r, "scaled:scale"))

# Confirm means and std devs are now 0 and 1
apply(x_test_r, 2, mean)
apply(x_test_r, 2, sd)
```

## Model building

### Create and define model

```{r}
input_r <- layer_input(shape = c(8))

output_r <- input_r %>% 
    layer_dense(units = 64, activation = 'relu') %>% 
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 32, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 1, activation = 'linear')
```

```{r}
model_r <- keras_model(inputs = input_r, 
                       outputs = output_r)

summary(model_r)
```

Compile model

```{r}
model_r %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('mean_squared_error'),
)
```

Model training

```{r}
# Shuffle the data manually before splitting into training and validation
set.seed(123)  # For reproducibility
shuffled_indices_r <- sample(1:nrow(x_train_r))

x_train_r <- x_train_r[shuffled_indices_r, ]
y_train_r <- y_train_r[shuffled_indices_r]

history_r <- model_r %>% fit(
  as.matrix(x_train_r), 
  as.matrix(y_train_r), 
  epochs = 50, 
  batch_size = 32, 
  validation_split = 0.2, 
  shuffle = TRUE,
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 10))
)
```

```{r}
y_test_hat_r <- model_r %>% predict(x_test_r)

preds_r <- data.frame(true_value = y_test_r, pred_value = y_test_hat_r)

preds_r <- preds_r %>% 
  mutate(abs_error = abs(true_value - pred_value))

preds_r
```

Evaluate model

```{r}
hist(preds_r$abs_error)
```

```{r}
options(repr.plot.width = 20, repr.plot.height = 8)

ggplot(preds_r, 
       aes(x = true_value, 
           y = pred_value)) +
    geom_point(size = 2) +
  labs(title = "Predicted value vs True value",
       x = "True value",
       y = "Predicted value") +
  theme_minimal() + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red")
```

# References

James, G., Witten, D., Hastie, T. and Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. New York: Springer.
