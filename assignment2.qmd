---
title: "STA5073 Assignment 2: Neural Networks"
format: pdf
author: "Jessica Stow (STWJES003@MYUCT.AC.ZA)"
date: "14 October 2024"
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE,
                      include = FALSE,
                      warning = FALSE)
```

# View this report on my GitHub!

You can access the repository for this report on [my GitHub profile](https://github.com/jessicastow/neural-networks).

# Plagiarism declaration

-   I know that plagiarism is wrong.

-   Plagiarism is to use another’s work and pretend that it is one’s own.

-   I have used the required convention for citation and referencing.

-   Each contribution to and quotation in this assignment from the work(s) of other people has been attributed, and has been cited and referenced.

-   This assignment is my own work.

-   I have not allowed, and will not allow, anyone to copy my work with the intention of passing it off as his or her own work.

-   I acknowledge that copying someone else’s assignment or essay, or part of it, is wrong, and declare that this is my own work.

\newpage

```{r}
library(reticulate)
reticulate::use_virtualenv("r-keras", required = TRUE)  
library(tidyverse)
library(keras3)
library(ggplot2)
```

# Introduction to Neural Networks

Neural networks, the cornerstone of deep learning, encompass a large class of models and machine learning methods. A neural network is a two-stage regression or classification model. They consist of an input layer, one or more hidden layers that utilise an activation function, which introduces non-linearity into the model to capture complex patterns in the data, and an output layer that produces the final prediction or classification result based on the learned representations (James et al. 2013).

# Multi-class Classification using Neural Networks

In this report, we utilise a neural network to perform multi-class classification and assess its suitability for the task. In neural networks, multi-class classification typically begins with an input layer, where the number of units corresponds to the number of features describing the data. This is followed by one or more hidden layers, which may include dense layers and dropout layers to prevent overfitting. Finally, the output layer contains a number of units equal to the possible classification classes, predicting the class to which each data point belongs.

## Objective

The objective of this task was to employ a neural network to predict a single target variable using a set of feature variables. The aim was to demonstrate the ability to apply neural networks as a suitable tool for addressing a classification problem.

## Data description

The dataset provided consisted of 21 numerical features, with the target variable being categorical and comprising five levels ("0", "1", "2", "3", and "4"). Limited information was available regarding the dataset features, which added a level of complexity when developing and fine-tuning the neural network to ensure accurate predictions.

```{r}
classification <- read.csv("data/Data-classification.csv")

str(classification[,1:21]) # Look at predictor variables

unique(classification$Target) # Look at target variable
```

\newpage

## Exploratory data analysis

### Check for duplicates and null values

Checks were done for the presence of duplicate entries and null values. No duplicate entries were found and there were no null values present.

```{r}
sum(duplicated(classification)) # no duplicates

sum(is.na(classification)) # no nulls
```

### Feature Distribution and Standardisation Recommendation

The spread of the data for the 21 variables was visualised in *Figure 1* below. It was evident that the ranges of these values varied significantly, with differing scales and distributions. This disparity in feature ranges could have impacted the performance of machine learning models, particularly in the application of neural networks, which are sensitive to the scale of input features. As a result, it was recommended that these features later be standardised to have a mean of 0 and a standard deviation of 1. This would allow the neural network model to converge more efficiently and ensure that all features contributed equally.

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(classification[,1:21], 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 1: Boxplot of 21 features for classification task")
```

### Exploring Feature Differences by Target Category

```{r}
classification$Target <- as.factor(classification$Target)
# Create the faceted box plot
ggplot(classification, 
       aes(x = Target, 
           y = X10)) +
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(x = "Target", y = "X1", title = "Box Plot of X1 by Target Category") +
  theme_minimal()
```

### Check for imbalances in target variable

It was observed that the target variable was highly imbalanced, as shown below in **Figure 2**. Imbalanced classes are not ideal, as they can lead to the model becoming biased in its predictions, with the model favouring the dominant class. This imbalance is evident in the dataset, where class 2 is significantly overrepresented with 3478 observations, while class 3, with only 310 observations, is underrepresented. Such disparity can result in the model performing poorly when identifying the minority class, which in this case is class 3.

Other concerns related to imbalanced datasets include the challenge of generalisation, as models trained on skewed data often struggle to perform well on unseen data. Additionally, models can become more unstable and less reliable. To mitigate the bias towards the larger classes, particularly class 2, undersampling techniques were applied to balance the dataset and improve the model's performance across all classes.

```{r}
# Check the frequency distribution of the Target column
table(classification$Target)

# Check the proportion of each Target category
prop.table(table(classification$Target))

# Convert Target to a factor if it is not already
classification$Target <- as.factor(classification$Target)
```

```{r include=TRUE, out.width="90%"}
# Visualise the distribution of the Target variable
ggplot(classification, 
       aes(x = Target)) +
  geom_bar(fill = "skyblue3", 
           color = "black") +
  labs(x = "Target", 
       y = "Count", 
       title = "Figure 2: Distribution of Target Categories") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
  
```

\newpage

## Data preparation

Once the data was loaded and explored, we applied the following preprocessing steps:

1.  **Addressing target variable imbalance through undersampling:** To correct the class imbalance in the target variable, we used undersampling.

-   We set a random seed for reproducibility, ensuring that the sampling process could be replicated for consistent results.

-   The dataset was balanced by randomly selecting 310 observations from each category, as this was the size of the smallest class.

-   By using undersampling, we created a balanced dataset where all target classes were equally represented. This approach helps improve the performance of our classification model by reducing bias towards the overrepresented classes.

```{r}
# Correct imbalance using undersampling
set.seed(123) # For reproducibility

# Balance the dataset so that each category in the Target has exactly 310 observations
classification_balanced <- classification %>%
  group_by(Target) %>% # Group by the Target variable
  sample_n(size = 310, replace = FALSE) %>%  # Sample 310 observations from each group, without replacement
  ungroup() # Ungroup to return a regular data frame
 
# Check the result to ensure each Target category has 310 observations
table(classification_balanced$Target) 
```

2.  **Creation of target variable and features:**

-   The target variable was separated from the feature set, where the target will serve as the predicted outcome, and the features will be used as inputs for modelling and further analysis.

```{r}
# Make target
classification_target <- classification_balanced$Target

# Make features
classification_features <- classification_balanced[,1:21]
```

4.  **Data split:** The dataset was split into training and testing sets using a 80-20 split ratio.

-   The data was split into the following shapes:

(a) Training set: 1248 observations, 21 features
(b) Testing set: 302 observations, 21 features

-   The training set would later be split into a training and validation set using a 80-20 split ratio.

```{r}
# Determine sample size
set.seed(123) # for reproducibility
ind <- sample(1:2, 
              nrow(classification_balanced), 
              replace=TRUE, 
              prob=c(0.8, 0.2))

# Split features
x_train <- classification_features[ind==1, ]
x_test <- classification_features[ind==2, ]

# Split target
y_train <- classification_target[ind==1]
y_test <- classification_target[ind==2]

# Check size 
nrow(x_train) # 1248 observations
nrow(x_test) # 302 observations
```

5.  **Feature scaling:**

-   The features used in the training data was scaled to have a mean of 0 and a standard deviation of 1. The test data was then scaled based on the training data means and standard deviations.

```{r}
# Scale training data
x_train <- scale(x_train)

# Confirm means and std devs are now 0 and 1
apply(x_train, 2, mean)
apply(x_train, 2, sd)

attributes(x_train) # previous means and sds stored here

# Scale test data
x_test <- scale(x_test, 
                center = attr(x_train, "scaled:center"), 
                scale = attr(x_train, "scaled:scale"))

# Confirm means and std devs are now 0 and 1
apply(x_test, 2, mean)
apply(x_test, 2, sd)
```

6.  **One-hot encoding:**

-   The integer-based target variable was transformed into a binary matrix using one-hot encoding, where each class is represented as its own binary variable, also known as "dummy variables".

```{r}
y_train <- keras3::to_categorical(y_train)
y_test_original <- y_test
y_test <- keras3::to_categorical(y_test)

# Check dimensions
dim(y_train)
```

## Multi-class classification model building

### Model definition and creation

A feedforward neural network was built to address this multi-class classification problem. This model consisted of an input layer, two dense layers and drop out layers, and one output layer. The layers used in this model were as follows:

-   The **input layer** had a shape of 21, since all 21 features were fed into the model as input.

-   The **first (hidden) dense layer** consisted of 64 neurons (units) and made use of the ReLU (Rectified Linear Unit) activation function - which introduces non-linearity into the model, allowing the model to learn complex patterns in the data it is trained on.

-   A **dropout layer** which randomly drops 20% of the neurons during training to prevent overfitting.

-   A **second dense layer** with 32 units and the ReLU activation function was then used. Having more than one dense layer enables the model to learn more complex representations of the data. The reason behind the reduction in the number of units (from 64 to 32) was to funnel the information learned by the model into a more precise form.

-   A **second dropout layer** was again used to reduce overfitting.

-   Finally, an **output layer** with 5 units, corresponding to the 5 classes in our target variable was used. For this layer, the softmax activation function was used since this is highly suitable for multi-class classification, since it is used to produce class probabilities. Softmax converts the output of the network into a probability distribution, where each of the 5 units will output a probability, and the sum of the probabilities across all units will equal 1. The class with the highest probability will be the predicted class.

```{r}
input <- layer_input(shape = c(21)) # input shape = 21 since we have 21 features

output <- input %>% 
    layer_dense(units = 64, activation = 'relu') %>% # can adjust units for complexity
    layer_dropout(rate = 0.2) %>% # can adjust drop out rate
    layer_dense(units = 32, activation = 'relu') %>%  # Additional layer
    layer_dropout(rate = 0.2) %>% 
    layer_dense(units = 5, # output for 5 classes (0-4)
                activation = 'softmax') # softmax activation function
```

The model was then defined by specifying the input and output layers. The intermediate dense and dropout layers were connected in a sequential manner.

```{r}
model <- keras_model(inputs = input, outputs = output)

summary(model)
```

### Model compilation

The model was compiled in preparation for the multi-class classification task. Categorical crossentropy was selected as the loss function to measure prediction error, as it is suited for multi-class problems. The Adam optimiser was used to adjust the model's weights, with a learning rate of 0.001, to minimise the loss function. Accuracy was chosen as the evaluation metric to assess the model's performance in predicting the correct class during both training and evaluation.

```{r}
# Configure model for training
model %>% compile(
  loss = 'categorical_crossentropy', # loss function for multi-class classification
  optimizer = optimizer_adam(learning_rate = 0.001), # optimizer instance = adam
  # evaluate the model (during training & testing) based on the following metrics:
  metrics =  c('accuracy')
)
```

### Model training

The data was first manually shuffled before splitting it into training and validation sets. This ensured the data was randomised. A set seed was used for reproducibility of the manual shuffle.

```{r}
# Shuffle the data manually before splitting into training and validation
set.seed(123)  # For reproducibility
shuffled_indices <- sample(1:nrow(x_train))

x_train <- x_train[shuffled_indices, ]
y_train <- y_train[shuffled_indices, ]
```

Once the training data had been shuffled, the model was trained using the `fit()` function. The model trained over the entire data set 50 times (number of epochs set to 50). This was done by dividing the training data into batches of 50 samples, and once the model had processed each batch, the model's weights were updated. This was done to help the model learn more efficiently as the updates to weights could be made more frequently instead of waiting for the entire data set to be trained on. Before each epoch, the training data was shuffled to ensure the model did not learn any patterns based on the order of the data.

20% of the data of the training data was used for validation during training. This allowed the model to be evaluated on unseen data after each epoch in order to monitor its performance and prevent overfitting.

Finally, an early stopping mechanism was included to monitor validation loss. Should the validation loss not improve for 10 consecutive epochs, the training will stop early. This is done to prevent overfitting and to save time by avoiding any unnecessary model training that does not improve the performance of the model.

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 50, # nr of times model trains on the entire data set
  batch_size = 50, # nr of samples processed before updating the model
  validation_split = 0.2, # using (the last) 20% of data for validation
  shuffle = TRUE, # shuffle training data before each epoch
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 10))
)

history # final
```
The two plots below show the model's performance during training and validation, using accuracy and loss across 50 epochs. The red lines indicate the performance of the training data, whereas the blue lines indicate the performance of the validation data.

**Accuracy:** The model's accuracy on the training data shows a steady improvement over the first 15-20 epochs, and stabilies close to 100% thereafter. This suggests the model is learning patterns in the training data well. Similarly, the validation accuracy also steadily increases as the training progresses, stabilising close to 100% after epoch ~20. This indicates the model generalises well to unseen data and does not tend to overfit. 

*Loss:** The training loss decreases steadily, indicating that the model's predictions on the training data are becoming more accurate. The model converges to a low loss value, which corresponds to the high accuracy observed.

Similarly, the validation loss decreases, with both curves closely following each other. The fact that the validation loss remains low and does not diverge from the training loss indicates that the model is not overfitting and is making reliable predictions on unseen data.

Overall, the insights from these plots indicate that the model does not show signs of overfitting (and thus generalises well to unseen data) and converges after approximately epoch 20 where the performance of both the training and validation sets stabilises. 


```{r include = TRUE, out.width="90%"}
# Manually add title later "Figure 3: Model Training and Validation Performance"
plot(history)
```

\newpage

### Model performance evaluation

The trained model's performance was evaluated on the test data.

The confusion matrix for the model predictions versus true classifications are displayed below in *Table 1*:

#### Table 1: Confusion Matrix

| **True Label / Predicted Label** | **0** | **1** | **2** | **3** | **4** |
|----------------------------------|-------|-------|-------|-------|-------|
| **0**                            | 55    | 1     | 0     | 0     | 0     |
| **1**                            | 0     | 69    | 0     | 1     | 0     |
| **2**                            | 0     | 0     | 59    | 0     | 0     |
| **3**                            | 2     | 0     | 1     | 54    | 0     |
| **4**                            | 0     | 0     | 0     | 0     | 60    |

\* *Note: rows represent true labels and columns represent predicted labels.*

Only 5 out of the 302 test predictions were misclassified, allowing for a high accuracy score of 98%.

The misclassifications for our test set were as follows:

-   For class 0, there was one observation misclassified as class 1.
-   For class 1, there was one observation misclassified as class 3.
-   For class 3, there were three observations misclassified. Two of the observations were misclassified as class 0 and one was misclassified as class 2. This class had the most misclassifications of all 5 classes.

For classes 2 and 4 there were no misclassifications.

```{r}
model %>% evaluate(x_test, y_test)

y_test_hat <- model %>% 
  predict(x_test) %>% 
  op_argmax(axis=2) %>% 
  as.numeric()

table(y_test_original, y_test_hat) # display confusion matrix
```

\newpage 

## Discussion

The implementation of a neural network for this multi-class classification challenge proved to be highly effective, as shown by the model's high accuracy scores on both the training and testing datasets. Following data preprocessing — which included addressing class imbalances and normalising the feature set — the model achieved an accuracy of 98% on the test set. This strong performance indicates the model's ability to generalise well on unseen data.

The confusion matrix further illustrates that the model performed well across all five classes, with only five misclassifications out of 302 test observations. Notably, the majority of misclassification errors were in class 3, which was the most underrepresented class in the original dataset. This suggests that while the undersampling method helped to balance the dataset, the model may have struggled to learn the distinguishing patterns associated with class 3 due to its limited representation.

The architecture of the neural network, which used ReLU activation functions in the hidden layers and a softmax activation function in the output layer, was well-suited for this classification task. The inclusion of dropout layers effectively mitigated overfitting, as demonstrated by stable performance on the validation set throughout training. Additionally, implementing early stopping ensured that the model did not overtrain, resulting in a more robust final model.

## Conclusion

In conclusion, the neural network model developed for this multi-class classification problem exhibited exceptional performance, achieving a high accuracy score and minimal misclassifications. The preprocessing steps, particularly those aimed at balancing the dataset through undersampling and feature scaling, are believed to have contributed to the model's effectiveness. However, some misclassifications persisted, particularly within the underrepresented class 3. This highlights an opportunity for further refinement in handling class imbalances.

## Recommendations

To further improve the model’s performance, it is recommended to explore alternative methods for addressing class imbalances. Techniques such as oversampling or synthetic data generation methods, such as SMOTE, could improve the model’s ability to effectively classify minority classes. Additionally, experimenting with different models, activation functions or hyperparameter tuning may yield further improvements. Overall, the results demonstrate the effectiveness and flexibility of neural networks in tackling complex classification tasks, even when faced with challenges such as data imbalance.

\newpage

# Regression task using Neural Networks

In this report, we utilise a neural network to perform regression and assess its suitability for the task. In neural networks, regression typically begins with an input layer, where the number of units corresponds to the number of features describing the data. This is followed by one or more hidden layers, which may include dense layers and dropout layers to prevent overfitting. Finally, the output layer contains one neuron and a linear activation function to predict the value of the data point using a combination of the features. 

## Objective

The objective of this task was to employ a neural network to predict a target variable using a set of 8 feature variables. The aim was to demonstrate the ability to apply neural networks as a suitable tool for addressing a regression problem.

## Data description

The dataset provided consisted of 8 numerical features, with the target variable being numeric. Limited information was available regarding the dataset features, which added a level of complexity when developing and fine-tuning the neural network to ensure accurate predictions.

```{r}
regression <- read.csv("data/Data-regression.csv")
str(regression)
```

\newpage 

## Exploratory Data Analysis

### Check for duplicates and null values

Checks were done for the presence of duplicate entries and null values. 25 duplicate entries were found and removed, and there were no null values present.

```{r}
sum(duplicated(regression)) # 25 duplicates

# drop duplicates
regression <- regression[!duplicated(regression), ]

# check duplicates were dropped
sum(duplicated(regression)) # 0 duplicates

sum(is.na(regression)) # no nulls
```

### Feature Distribution and Standardisation Recommendation

The spread of the data for the 8 variables was visualised in *Figure 4* below. It was evident that the ranges of these values varied significantly, with differing scales and distributions. This disparity in feature ranges could have impacted the performance of machine learning models, particularly in the application of neural networks, which are sensitive to the scale of input features. As a result, it was recommended that these features later be standardised to have a mean of 0 and a standard deviation of 1. This would allow the neural network model to converge more efficiently and ensure that all features contributed equally.

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(regression[,1:8], 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 4: Boxplot of 8 features for regression task")
```

### Spread of target variable

The target variable has values across a wide range (2.33 to 82.6) which include a handful of outliers on the upper end of the distribution. *Figure 5* below shows the mostly uniform distribution of the target variable used in this regression task.

```{r}
min(regression$target)
max(regression$target)
```

```{r include = TRUE, out.width="90%"}
# Boxplot of spread of data
par(las=2, # rotate horizontal axis labels
    cex.axis = 0.6)

boxplot(regression$target, 
        main = "", 
        horizontal = TRUE)

title(main = "Figure 5: Boxplot of target variable")
```

## Data preparation

1.  **Create features and target:**

-   The target variable was separated from the feature set, where the target will serve as the predicted outcome, and the features will be used as inputs for modelling and further analysis.

```{r}
regression_features <- regression[,1:8]
regression_target <- regression$target
```

2.  **Data split:** The dataset was split into training and testing sets using a 80-20 split ratio.

-   The data was split into the following shapes:

(a) Training set: 850 observations, 8 features
(b) Testing set: 200 observations, 8 features

-   The training set would later be split into a training and validation set using a 80-20 split ratio.

```{r}
# Determine sample size
set.seed(123) # for reproducibility
ind <- sample(1:2, 
              nrow(regression), 
              replace=TRUE, 
              prob=c(0.8, 0.2))

# Split features
x_train_r <- regression_features[ind==1, ]
x_test_r <- regression_features[ind==2, ]

# Split target
y_train_r <- regression_target[ind==1]
y_test_r <- regression_target[ind==2]

# Check size 
nrow(x_train_r) # 850 observations
nrow(x_test_r) # 200 observations
```

**Feature scaling:**

-   The features used in the training data was scaled to have a mean of 0 and a standard deviation of 1. The test data was then scaled based on the training data means and standard deviations.

```{r}
# Scale training data
x_train_r <- scale(x_train_r)

# Confirm means and std devs are now 0 and 1
apply(x_train_r, 2, mean)
apply(x_train_r, 2, sd)

attributes(x_train_r) # previous means and sds stored here

# Scale test data
x_test_r <- scale(x_test_r, 
                center = attr(x_train_r, "scaled:center"), 
                scale = attr(x_train_r, "scaled:scale"))

# Confirm means and std devs are now 0 and 1
apply(x_test_r, 2, mean)
apply(x_test_r, 2, sd)
```

## Regression model building

### Model definition and creation

A feedforward neural network was built to address this regression problem. This model consisted of an input layer, two dense layers and drop out layers, and one output layer. The layers used in this model were as follows:

-   The **input layer** had a shape of 8, since all 8 features were fed into the model as input.

-   The **first (hidden) dense layer** consisted of 64 neurons (units) and made use of the ReLU (Rectified Linear Unit) activation function - which introduces non-linearity into the model, allowing the model to learn complex patterns in the data it is trained on.

-   A **dropout layer** which randomly drops 20% of the neurons during training to prevent overfitting.

-   A **second dense layer** with 32 neurons (units) and the ReLU activation function was then used. Having more than one dense layer enables the model to learn more complex representations of the data. The reason behind the reduction in the number of units (from 64 to 32) was to funnel the information learned by the model into a more precise form.

-   A **second dropout layer** was again used to reduce overfitting.

-   The **output layer** consists of one neuron, which produces the predicted outcome for the regression task. Since this is a regression problem, the output is a continuous value, and a linear activation function is used. The linear activation allows the model to predict a range of values without constraints.

```{r}
input_r <- layer_input(shape = c(8))

output_r <- input_r %>% 
    layer_dense(units = 64, activation = 'relu') %>% 
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 32, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 1, activation = 'linear')
```

The model was then defined by specifying the input and output layers. The intermediate dense and dropout layers were connected in a sequential manner.

```{r}
model_r <- keras_model(inputs = input_r, 
                       outputs = output_r)

summary(model_r)
```

### Model compilation

The model was compiled in preparation for the regression task, with mean squared error (MSE) chosen as the loss function to measure prediction error, as it is well-suited to regression problems. The Adam optimiser was selected to update the model's weights, using a learning rate of 0.01. This learning rate is small enough to steadily reduce the loss function without causing abrupt weight changes. Mean squared error was used as the evaluation metric during both training and testing phases.

```{r}
model_r %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = c('mean_squared_error'),
)
```

### Model training

The data was manually shuffled before being split into training and validation sets to ensure randomisation and eliminate any potential patterns based on the original order of the data. A random seed was set to ensure the shuffle process could be reproduced consistently.

```{r}
# Shuffle the data manually before splitting into training and validation
set.seed(123)  # For reproducibility
shuffled_indices_r <- sample(1:nrow(x_train_r))

x_train_r <- x_train_r[shuffled_indices_r, ]
y_train_r <- y_train_r[shuffled_indices_r]
```

Once the training data had been shuffled, the model was trained using the `fit()` function. The model trained over the entire data set 50 times (number of epochs set to 50). This was done by dividing the training data into batches of 32 samples, and once the model had processed each batch, the model's weights were updated. This was done to help the model learn more efficiently as the updates to weights could be made more frequently instead of waiting for the entire data set to be trained on. Before each epoch, the training data was shuffled to ensure the model did not learn any patterns based on the order of the data.

20% of the data of the training data was used for validation during training. This allowed the model to be evaluated on unseen data after each epoch in order to monitor its performance and prevent overfitting.

Finally, an early stopping mechanism was included to monitor validation loss. Should the validation loss not improve for 10 consecutive epochs, the training will stop early. This is done to prevent overfitting and to save time by avoiding any unnecessary model training that does not improve the performance of the model.

```{r cache=TRUE}
set.seed(123)
history_r <- model_r %>% fit(
  as.matrix(x_train_r), 
  as.matrix(y_train_r), 
  epochs = 50, 
  batch_size = 32, 
  validation_split = 0.2, 
  shuffle = TRUE,
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 10))
)

history_r
```
The two plots below show the model's performance during training and validation, using mean squared error and loss across 50 epochs. The red lines indicate the performance of the training data, whereas the blue lines indicate the performance of the validation data.

**Loss:** The training loss starts high, decreases quickly during the first 15 epochs, and then plateaus around 90. This indicates that the model is learning and minimising errors on the training data over time.
  
The validation loss follows a similar trend to the training loss, decreasing steadily and stabilising after approximately 10 epochs. This indicates that the model generalises well to unseen data (validation set), as there is a small gap between training and validation loss.

**MSE:** The mean squared error for the training data decreases rapidly at first and then stabilises after 15 epochs, indicating that the model's predictions are becoming closer to the true values as training progresses.
  
The validation MSE closely follows the same trend as the training MSE, which is a good sign that the model is not overfitting. The validation MSE stabilises at a low value of around 10 epochs, further suggesting that the model generalises well to the validation data.

Overall, the training and validation curves for both loss and MSE are closely aligned, indicating that the model is not overfitting. If the validation curves had diverged significantly from the training curves, it would suggest overfitting.
  
The similar trends and values for the training and validation sets show that the model is performing well on both the data it was trained on and the unseen validation data, suggesting good generalisation to new data.

In conclusion, the plot shows that the model is effectively learning, with both training and validation loss and MSE decreasing and stabilising at low values, and no significant overfitting is evident.

\\
\\
\\

```{r include=TRUE, out.width="90%", cache=TRUE}
plot(history_r)
```

\newpage

### Model performance and evaluation 

The performance of the trained model was evaluated using the test data, resulting in a mean squared error (MSE) of 53.19.

```{r}
# Predict using the model
y_test_hat_r <- model_r %>% predict(x_test_r)

# Create a dataframe of true and predicted values
preds_r <- data.frame(true_value = y_test_r, pred_value = y_test_hat_r)

# Calculate squared errors
preds_r <- preds_r %>% 
  mutate(
    squared_error = (true_value - pred_value)^2
  )

# Calculate MSE
mean(preds_r$squared_error) # 53.19
```

The predicted versus actual values were visualised in *Figure 6* below. When comparing the predicted values to the true values, the model shows a tendency to overpredict when the true value is less than 20, and to underpredict when the true values exceed 60. Overall, the predictions align reasonably well with the true values, as indicated by the points clustering around the red dashed line (which represents perfect predictions). However, there is noticeable variation in the predictions, with some points deviating from the ideal line, indicating that while the model captures the general trend, it struggles to predict extreme values accurately.

```{r include=TRUE}
ggplot(preds_r, 
       aes(x = true_value, 
           y = pred_value)) +
    geom_point(size = 2) +
  labs(title = "Figure 6: Predicted values vs True values",
       x = "True value",
       y = "Predicted value") +
  theme_minimal() + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  coord_fixed()  # Make the plot square
```

## Discussion

In this task, a feedforward neural network was employed to address a regression problem using 8 input features to predict a continuous target variable. The data underwent thorough preprocessing, including the removal of duplicates and the scaling of features to ensure better model convergence and performance. The model architecture consisted of two dense hidden layers with ReLU activation functions, interspersed with dropout layers to prevent overfitting, followed by a single-neuron output layer with a linear activation function suitable for regression tasks.

During training, the model exhibited a rapid decrease in both loss and mean squared error (MSE) over the first 15 epochs, after which it stabilised. The validation performance closely followed the training performance, with minimal divergence between the training and validation curves. This suggests that the model was able to generalise well to unseen data, without significant overfitting. The application of dropout layers and early stopping helped to ensure that the model did not overfit to the training data, further evidenced by the stabilisation of the validation loss and MSE.

Despite the model’s generally strong performance, the evaluation on the test data revealed some limitations. The mean squared error on the test set was 53.19, indicating a reasonable level of accuracy, though there were noticeable deviations in predictions, particularly for extreme values. The model tended to overpredict when the true values were below 20 and underpredict when they exceeded 60. While the model captured the overall trend, it struggled with outliers or extreme cases, suggesting that further tuning or a more complex model might be necessary to improve performance in such scenarios.

## Conclusion

The neural network successfully addressed the regression task, demonstrating strong generalisation to the validation and test data. The model was able to minimise both training and validation errors effectively, thanks to the use of dropout and early stopping mechanisms. The mean squared error of 53.19 on the test set highlights the model's capacity to predict reasonably well, though its difficulty in handling extreme values suggests that there is room for improvement. Future work could explore alternative architectures or hyperparameter tuning to further enhance the model’s predictive performance, particularly in relation to edge cases and outliers. Overall, the neural network proved to be a robust tool for this regression problem, showcasing its flexibility and effectiveness in handling complex, multi-feature datasets.

# References

James, G., Witten, D., Hastie, T. and Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. New York: Springer.
